{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800306ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install PyAthena[SQLAlchemy]\n",
    "!pip install CurrencyConverter\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from sqlalchemy import create_engine\n",
    "from currency_converter import CurrencyConverter, ECB_URL\n",
    "import tr_dash_util as util\n",
    "from tr_dash_util import clean_eReg_dataframe\n",
    "\n",
    "# Define constants and configurations\n",
    "S3_STAGING_DIR = \"s3://ets-aws-plalab-dii-prod-analyticsbucket-1ktrlhzbrcbkb/athena_query_results/\"\n",
    "ATHENA_REGION = \"us-east-1\"\n",
    "APPLICATION_ID = '01845c7c-fa6d-4788-9ddd-cd888c977f36'\n",
    "ENV = 'prod'\n",
    "USER_TYPE = 'external_user'\n",
    "EVENT_NAMES = ['ProfileCreated', 'OrderSubmitted']\n",
    "START_DATE = datetime.datetime(2024, 2, 1)\n",
    "END_DATE = datetime.datetime(2024, 2, 10)\n",
    "EXTENDED_END_DATE = datetime.datetime(2024, 5, 27)  # New end date\n",
    "\n",
    "# SKU lists\n",
    "NON_PREP_SKUS = [\n",
    "    '-2002', '-2001', '-1111', '4001', '4002', '4003', '4004', '4005', '4006', '4007', '4008', '4009', '4010', '4011', \n",
    "    '4012', '4013', '4014', '4015', '4016', '4017', '4018', '4019', '4020', '4028', '4034', '4035', '4091', '4092', '4093', \n",
    "    '4321', '5000', '5001', '5002', '5003', '5004', '5005', '5006', '5007', '5009', '5010', '5020', '5021', '5022', '5023', \n",
    "    '5024', '5025', '5026', '5027', '5028', '5029', '5030', '5031', '5032', '5033', '5035', '5037', '5040', '5041', '5042', \n",
    "    '5043', '5044', '5045', '5046', '5047', '5048', '5049', '5050', '5051', '5052', '5053', '5054', '5055', '5056', '5057', \n",
    "    '5058', '5059', '5060', '5061', '5062', '5063', '5064', '5065', '5066', '5067', '5068', '5069', '5070', '5071', '5072', \n",
    "    '5073', '5074', '5075', '5076', '5077', '5078', '5079', '5080', '5081', '5082', '5083', '5084', '5085', '5086', '5087', \n",
    "    '5088', '5089', '5091', '5093', '5094', '5095', '5096', '5097', '5098', '5099', '5100', '5101', '5102', '5103', '5104', \n",
    "    '5105', '5106', '5107', '5108', '5109', '5110', '5111', '5112', '5113', '5114', '5115', '5116', '5117', '5118', '5119', \n",
    "    '5120', '5121', '5122', '5123', '5124', '5125', '5126', '5127', '5128', '5129', '5130', '5131', '5132', '5133', '5134', \n",
    "    '5135', '5136', '5137', '5138', '5139', '5140', '5142', '5143', '5144', '5145', '5146', '5147', '5148', '5149', '5150', \n",
    "    '5151', '5152', '5154', '5155', '5156', '5157', '5159', '5160', '5161', '5162', '5163', '5164', '5165', '5166', '5167', \n",
    "    '5168', '5169', '5170', '5171', '5172', '5173', '5174', '5175', '5176', '5177', '5178', '5179', '5180', '5181', '5182', \n",
    "    '5183', '5184', '5185', '5186', '5187', '5188', '5189', '5190', '5191', '5192', '5193', '5194', '5195', '5196', '5197', \n",
    "    '5198', '5199', '5200', '5600', '5601', '5602', '5603', '5604', '5605', '5606', '5607', '5608', '5609', '5610', '5611', \n",
    "    '5612', '5613', '5614', '5615', '5616', '5617', '5618', '5619', '5620', '5621', '5622', '5624', '5625', '5626', '5627', \n",
    "    '5628', '5629', '5630', '5631', '5632', '5633', '5634', '5635', '5636', '5637', '5638', '5639', '5640', '5641', '5642', \n",
    "    '5643', '5644', '5645', '5646', '5647', '5648', '5649', '5650', '5651', '5652', '5653', '5654', '5655', '5656', '5657', \n",
    "    '5658', '5659', '5660', '5661', '5662', '5663', '5664', '5665', '5666', '5667', '5668', '5669', '5670', '5671', '5672', \n",
    "    '5673', '5674', '5675', '5676', '5677', '5678', '5679', '5680', '5681', '5682', '5683', '5684', '5685', '5686', '5687', \n",
    "    '5688', '5689', '5690', '5691', '5692', '5693', '5694', '5695', '5696', '6001', '6002', '6003', '6004', '6005', '6009', \n",
    "    '6010', '6011', '6012', '6013', '6014', '6015', '6016', '6017', '6018', '100155', '100156', '100206', '100207'\n",
    "]\n",
    "SKU_ID_REGISTRATION = ['4001', '4017']\n",
    "\n",
    "# Function to create engine connection to Athena\n",
    "def create_engine_connection():\n",
    "    connection_string = f\"awsathena+rest://:@athena.{ATHENA_REGION}.amazonaws.com:443/labsprodeventsdatabase-x806vjuzpbrd?s3_staging_dir={S3_STAGING_DIR}\"\n",
    "    engine = create_engine(connection_string)\n",
    "    return engine\n",
    "\n",
    "# Function to fetch data for a specific date from Athena\n",
    "def fetch_data_for_date(engine, date):\n",
    "    year, month, day = date.strftime(\"%Y\"), date.strftime(\"%m\"), date.strftime(\"%d\")\n",
    "    query = f\"\"\"\n",
    "    SELECT * FROM processed_events\n",
    "    WHERE application_id='{APPLICATION_ID}'\n",
    "    AND year='{year}' AND month='{month}' AND day='{day}'\n",
    "    AND env = '{ENV}' AND user_type = '{USER_TYPE}'\n",
    "    AND event_name IN ('ProfileCreated', 'OrderSubmitted')\n",
    "    \"\"\"\n",
    "    df = pd.read_sql(query, engine)\n",
    "    df_clean = clean_eReg_dataframe(df)\n",
    "    return df_clean\n",
    "\n",
    "# Function to generate a range of dates\n",
    "def daterange(start_date, end_date):\n",
    "    for n in range(int((end_date - start_date).days) + 1):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "# Function to extract SKU IDs from event data\n",
    "def get_sku_id_list(df):\n",
    "    \"\"\"\n",
    "    Extracts SKU IDs from order data, including user ID, order ID, \n",
    "    order submission time, and SKU ID list for each order.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame containing event data.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: A DataFrame containing user ID, order ID, order submission time, \n",
    "               and SKU ID list for each order.\n",
    "    \"\"\"\n",
    "    # Define a list to hold our processed rows\n",
    "    rows = []\n",
    " \n",
    "    # Iterate through each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Load the JSON content from 'event_data' column\n",
    "        event_data = json.loads(row['event_data'])\n",
    "        # Extract necessary data\n",
    "        user_id = row['user_id']\n",
    "        order_id = event_data.get('data', {}).get('orderid')\n",
    "        order_submit_time = row['event_timestamp']\n",
    "        # Extract all SKU IDs from order items\n",
    "        sku_ids = [item.get('skuid') for item in event_data.get('data', {}).get('orderitems', [])]\n",
    "        # Append the extracted data as a dictionary to the rows list\n",
    "        rows.append({\n",
    "            'user_id': user_id,\n",
    "            'order_id': order_id,\n",
    "            'order_submit_time': order_submit_time,\n",
    "            'sku_id_list': sku_ids\n",
    "        })\n",
    "    # Create a DataFrame from the rows list\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n",
    "\n",
    "# Function to update order DataFrame with specific SKU filters\n",
    "def update_order_df(df, non_prep_SKUs, sku_id_registration):\n",
    "    \"\"\"\n",
    "    Updates the order DataFrame by filtering out non-preparatory SKUs, \n",
    "    sorting orders within each user by submission time, and keeping only \n",
    "    the first order for each user. It also adds profile creation time and \n",
    "    time difference in seconds to the DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The input DataFrame containing event data.\n",
    "    non_prep_skus (list): List of non-preparatory SKU IDs.\n",
    "    sku_id_registration (list): List of SKU IDs for registration.\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: The updated DataFrame with filtered and sorted orders.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Convert SKU lists to integers, filtering out non-integer strings\n",
    "    non_prep_SKUs_int = [int(sku) for sku in non_prep_SKUs if sku.isdigit()]\n",
    "    sku_id_registration_int = [int(sku) for sku in sku_id_registration if sku.isdigit()]\n",
    "    # Convert lists to sets for operations\n",
    "    non_prep_SKUs_set = set(non_prep_SKUs_int)\n",
    "    sku_id_registration_set = set(sku_id_registration_int)\n",
    "    # Compute the difference to find sku_id_fees\n",
    "    sku_id_fees = non_prep_SKUs_set - sku_id_registration_set\n",
    " \n",
    "    # Extract profile_created_time for each user\n",
    "    profile_created_times = {}\n",
    "    profile_created_rows = df[df['event_name'] == 'ProfileCreated']\n",
    "    for index, row in profile_created_rows.iterrows():\n",
    "        user_id = row['user_id']\n",
    "        profile_created_time = row['event_timestamp']\n",
    "        profile_created_times[user_id] = profile_created_time\n",
    " \n",
    "    # Log missing user_ids for debugging\n",
    "    missing_user_ids = []\n",
    " \n",
    "    # List to collect rows for the new DataFrame\n",
    "    updated_rows = []\n",
    "    # Iterate through each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Process only rows where event_name is OrderSubmitted\n",
    "        if row['event_name'] != 'OrderSubmitted':\n",
    "            continue\n",
    "        # Load the JSON content from 'event_data' column\n",
    "        event_data = json.loads(row['event_data'])\n",
    "        # Extract necessary data\n",
    "        user_id = row['user_id']\n",
    "        order_id = event_data.get('data', {}).get('orderid')\n",
    "        order_submit_time = row['event_timestamp']\n",
    "        sku_ids = [int(item.get('skuid')) for item in event_data.get('data', {}).get('orderitems', []) if item.get('skuid', '')]\n",
    "        # Filter out the SKUs that are only fee-related, leaving other types of SKUs\n",
    "        filtered_sku_ids = [sku for sku in sku_ids if sku not in sku_id_fees]\n",
    "        # Skip rows with empty SKU ID lists after filtering\n",
    "        if not filtered_sku_ids:\n",
    "            continue\n",
    "        # Check if any SKU ID is a registration SKU\n",
    "        registration_marker = 1 if any(sku in sku_id_registration_set for sku in filtered_sku_ids) else 0\n",
    "        # Get profile_created_time for the user\n",
    "        profile_created_time = profile_created_times.get(user_id)\n",
    "        if profile_created_time is None:\n",
    "            missing_user_ids.append(user_id)\n",
    "        # Calculate time_diff_sec if profile_created_time is available\n",
    "        time_diff_sec = None\n",
    "        if profile_created_time is not None:\n",
    "            time_diff_sec = (order_submit_time - profile_created_time) / 1e6  # Convert from microseconds to seconds\n",
    "        # Append the processed data\n",
    "        updated_rows.append({\n",
    "            'user_id': user_id,\n",
    "            'order_id': order_id,\n",
    "            'order_submit_time': order_submit_time,\n",
    "            'sku_id_list': filtered_sku_ids,\n",
    "            'registration_marker': registration_marker,\n",
    "            'profile_created_time': profile_created_time,\n",
    "            'time_diff_sec': time_diff_sec\n",
    "        })\n",
    "    \n",
    "    # Create a DataFrame from the updated rows\n",
    "    updated_df = pd.DataFrame(updated_rows)\n",
    "    # Filter to keep only rows where registration_marker is 0\n",
    "    updated_df = updated_df[updated_df['registration_marker'] == 0]\n",
    "    # Sort the DataFrame first by 'user_id', then by 'order_submit_time'\n",
    "    updated_df.sort_values(by=['user_id', 'order_submit_time'], ascending=[True, True], inplace=True)\n",
    "    # Keep only the first order for each user\n",
    "    updated_df = updated_df.drop_duplicates(subset='user_id', keep='first')\n",
    "    return updated_df\n",
    "\n",
    "# Main script execution\n",
    "if __name__ == \"__main__\":\n",
    "    engine = create_engine_connection()\n",
    "    data_frames = []\n",
    "    \n",
    "    for single_date in daterange(START_DATE, EXTENDED_END_DATE):\n",
    "        print(f\"Fetching data for {single_date.strftime('%Y-%m-%d')}...\")\n",
    "        df_clean = fetch_data_for_date(engine, single_date)\n",
    "        data_frames.append(df_clean)\n",
    "    \n",
    "    full_data_frame = pd.concat(data_frames, ignore_index=True)\n",
    "    df_updated = update_order_df(full_data_frame, NON_PREP_SKUS, SKU_ID_REGISTRATION)\n",
    "    \n",
    "    # Save the updated DataFrame as a parquet file\n",
    "    df_updated.to_parquet('updated_orders.parquet')\n",
    "    \n",
    "    # Save the updated DataFrame as a CSV file\n",
    "    df_updated.to_csv('updated_orders.csv', index=False)\n",
    "\n",
    "    # Optionally display the DataFrame\n",
    "    import ace_tools as tools; tools.display_dataframe_to_user(name=\"Updated Orders DataFrame\", dataframe=df_updated)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
