{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec84bf7e-e5f7-4e7f-a518-0a303856cace",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyAthena[SQLAlchemy] in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (3.8.3)\n",
      "Requirement already satisfied: boto3>=1.26.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from PyAthena[SQLAlchemy]) (1.34.142)\n",
      "Requirement already satisfied: botocore>=1.29.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from PyAthena[SQLAlchemy]) (1.34.142)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from PyAthena[SQLAlchemy]) (2024.6.0)\n",
      "Requirement already satisfied: tenacity>=4.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from PyAthena[SQLAlchemy]) (8.3.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from PyAthena[SQLAlchemy]) (2.0.30)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3>=1.26.4->PyAthena[SQLAlchemy]) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3>=1.26.4->PyAthena[SQLAlchemy]) (0.10.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore>=1.29.4->PyAthena[SQLAlchemy]) (2.9.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore>=1.29.4->PyAthena[SQLAlchemy]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sqlalchemy>=1.0.0->PyAthena[SQLAlchemy]) (4.12.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sqlalchemy>=1.0.0->PyAthena[SQLAlchemy]) (3.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.29.4->PyAthena[SQLAlchemy]) (1.16.0)\n",
      "Requirement already satisfied: CurrencyConverter in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.17.28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5560/3269997067.py:27: SADeprecationWarning: The dbapi() classmethod on dialect classes has been renamed to import_dbapi().  Implement an import_dbapi() classmethod directly on class <class 'pyathena.sqlalchemy.rest.AthenaRestDialect'> to remove this warning; the old .dbapi() classmethod may be maintained for backwards compatibility.\n",
      "  engine = create_engine(connection_string)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size before merging:\n",
      "free_test_section: 1802\n",
      "target: 6000\n",
      "modifile_free_test: 1921\n",
      "modifile_max_streak: 4494\n",
      "modifile_study_plan: 4336\n",
      "page_view: 4953\n",
      "paid_prep: 1948\n",
      "view_feedback: 2061\n",
      "view_plan: 1556\n",
      "Sample size after merging: 6000\n",
      "Final merged data saved to: merged_dataset_final.csv\n"
     ]
    }
   ],
   "source": [
    "# Install necessary packages\n",
    "!pip install PyAthena[SQLAlchemy]\n",
    "!pip install CurrencyConverter\n",
    "\n",
    "# Import necessary libraries\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "from currency_converter import CurrencyConverter, ECB_URL\n",
    "import numpy as np\n",
    "import html\n",
    "import json\n",
    "import re\n",
    "from functools import reduce\n",
    "import pytz\n",
    "\n",
    "# Set time and environment\n",
    "today = datetime.now() - timedelta(days=1)\n",
    "prod = True\n",
    "\n",
    "# S3 path for Athena\n",
    "s3_staging_dir = \"s3://ets-aws-plalab-dii-prod-analyticsbucket-1ktrlhzbrcbkb/athena_query_results/\"\n",
    "\n",
    "# Athena connection string\n",
    "connection_string = f\"awsathena+rest://:@athena.us-east-1.amazonaws.com:443/labsprodeventsdatabase-x806vjuzpbrd?s3_staging_dir={s3_staging_dir}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Function to pull Parquet files from S3\n",
    "def pull_parquet_files_pandas_Target_features(file_name, bucket_name='ets-dii-testready-ds-analytics'):\n",
    "    s3_client = boto3.client('s3')\n",
    "    prefix = f'Eben/Features/file_name={file_name}/'\n",
    "\n",
    "    def list_parquet_files(bucket, prefix):\n",
    "        file_paths = []\n",
    "        paginator = s3_client.get_paginator('list_objects_v2')\n",
    "        for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "            for content in page.get('Contents', []):\n",
    "                key = content.get('Key')\n",
    "                if key.endswith('.parquet'):\n",
    "                    file_paths.append(f\"s3://{bucket}/{key}\")\n",
    "        return file_paths\n",
    "\n",
    "    file_paths = list_parquet_files(bucket_name, prefix)\n",
    "    df = pd.DataFrame()\n",
    "    for file_path in file_paths:\n",
    "        temp_df = pd.read_parquet(file_path, engine='pyarrow')\n",
    "        df = pd.concat([df, temp_df], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# Load the datasets\n",
    "free_test_section = pull_parquet_files_pandas_Target_features(file_name='free_test_section')\n",
    "target = pull_parquet_files_pandas_Target_features(file_name='Target')\n",
    "modifile_free_test = pull_parquet_files_pandas_Target_features(file_name='modifile_free_test')\n",
    "modifile_max_streak = pull_parquet_files_pandas_Target_features(file_name='modifile_max_streak')\n",
    "modifile_study_plan = pull_parquet_files_pandas_Target_features(file_name='modifile_study_plan')\n",
    "page_view = pull_parquet_files_pandas_Target_features(file_name='page_view')\n",
    "paid_prep = pull_parquet_files_pandas_Target_features(file_name='paid_prep')\n",
    "view_feedback = pull_parquet_files_pandas_Target_features(file_name='view_feedback')\n",
    "view_plan = pull_parquet_files_pandas_Target_features(file_name='view_plan')\n",
    "\n",
    "# Check sample sizes before merging\n",
    "print(\"Sample size before merging:\")\n",
    "datasets = {\n",
    "    \"free_test_section\": free_test_section,\n",
    "    \"target\": target,\n",
    "    \"modifile_free_test\": modifile_free_test,\n",
    "    \"modifile_max_streak\": modifile_max_streak,\n",
    "    \"modifile_study_plan\": modifile_study_plan,\n",
    "    \"page_view\": page_view,\n",
    "    \"paid_prep\": paid_prep,\n",
    "    \"view_feedback\": view_feedback,\n",
    "    \"view_plan\": view_plan\n",
    "}\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    print(f\"{name}: {len(df)}\")\n",
    "\n",
    "# Merge the datasets with explicit suffixes to avoid conflicts\n",
    "merged_df = target.merge(free_test_section, on='user_id', how='left', suffixes=('', '_free_test_section')) \\\n",
    "                  .merge(modifile_free_test, on='user_id', how='left', suffixes=('', '_modifile_free_test')) \\\n",
    "                  .merge(modifile_max_streak, on='user_id', how='left', suffixes=('', '_modifile_max_streak')) \\\n",
    "                  .merge(modifile_study_plan, on='user_id', how='left', suffixes=('', '_modifile_study_plan')) \\\n",
    "                  .merge(page_view, on='user_id', how='left', suffixes=('', '_page_view')) \\\n",
    "                  .merge(paid_prep, on='user_id', how='left', suffixes=('', '_paid_prep')) \\\n",
    "                  .merge(view_feedback, on='user_id', how='left', suffixes=('', '_view_feedback')) \\\n",
    "                  .merge(view_plan, on='user_id', how='left', suffixes=('', '_view_plan'))\n",
    "\n",
    "# Drop the unwanted columns while retaining COUNTRY\n",
    "columns_to_drop = [col for col in merged_df.columns if re.search(r'country_code|first_date|country(?!_name)', col)]\n",
    "merged_df = merged_df.drop(columns=columns_to_drop)\n",
    "\n",
    "# Check sample size after merging\n",
    "print(f\"Sample size after merging: {len(merged_df)}\")\n",
    "\n",
    "# Save the final merged dataframe to a CSV file\n",
    "final_csv_path = 'merged_dataset_final.parquet'\n",
    "merged_df.to_parquet(final_csv_path, index=False)\n",
    "\n",
    "# Save the final merged dataframe to a CSV file\n",
    "final_csv_path = 'merged_dataset_final.csv'\n",
    "merged_df.to_csv(final_csv_path, index=False)\n",
    "\n",
    "print(f\"Final merged data saved to: {final_csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
