{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55e1ab7c-7ef6-4570-b631-2e64ab4c09b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyAthena[SQLAlchemy] in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (3.8.3)\n",
      "Requirement already satisfied: boto3>=1.26.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from PyAthena[SQLAlchemy]) (1.34.142)\n",
      "Requirement already satisfied: botocore>=1.29.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from PyAthena[SQLAlchemy]) (1.34.142)\n",
      "Requirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from PyAthena[SQLAlchemy]) (2024.6.0)\n",
      "Requirement already satisfied: tenacity>=4.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from PyAthena[SQLAlchemy]) (8.3.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from PyAthena[SQLAlchemy]) (2.0.30)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3>=1.26.4->PyAthena[SQLAlchemy]) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3>=1.26.4->PyAthena[SQLAlchemy]) (0.10.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore>=1.29.4->PyAthena[SQLAlchemy]) (2.9.0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore>=1.29.4->PyAthena[SQLAlchemy]) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sqlalchemy>=1.0.0->PyAthena[SQLAlchemy]) (4.12.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sqlalchemy>=1.0.0->PyAthena[SQLAlchemy]) (3.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore>=1.29.4->PyAthena[SQLAlchemy]) (1.16.0)\n",
      "us-east-1\n",
      "Requirement already satisfied: CurrencyConverter in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (0.17.28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18179/4002347918.py:28: SADeprecationWarning: The dbapi() classmethod on dialect classes has been renamed to import_dbapi().  Implement an import_dbapi() classmethod directly on class <class 'pyathena.sqlalchemy.rest.AthenaRestDialect'> to remove this warning; the old .dbapi() classmethod may be maintained for backwards compatibility.\n",
      "  engine = create_engine(connection_string)\n",
      "/tmp/ipykernel_18179/4002347918.py:152: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  events_df['date'] = pd.to_datetime(events_df['date'])\n",
      "/tmp/ipykernel_18179/4002347918.py:152: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  events_df['date'] = pd.to_datetime(events_df['date'])\n",
      "/tmp/ipykernel_18179/4002347918.py:152: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  events_df['date'] = pd.to_datetime(events_df['date'])\n",
      "/tmp/ipykernel_18179/4002347918.py:152: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  events_df['date'] = pd.to_datetime(events_df['date'])\n",
      "/tmp/ipykernel_18179/4002347918.py:152: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  events_df['date'] = pd.to_datetime(events_df['date'])\n",
      "/tmp/ipykernel_18179/4002347918.py:152: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  events_df['date'] = pd.to_datetime(events_df['date'])\n",
      "/tmp/ipykernel_18179/4002347918.py:152: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  events_df['date'] = pd.to_datetime(events_df['date'])\n",
      "/tmp/ipykernel_18179/4002347918.py:152: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  events_df['date'] = pd.to_datetime(events_df['date'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified Aggregated Study Plan Data:\n",
      "       user_id                 first_date country  number_of_study_plans\n",
      "0     13F987F6 2024-02-01 12:05:38.203954     CHN                      3\n",
      "1     13F98A7A 2024-02-01 12:05:38.203954     CAN                      1\n",
      "2     13F98AHC 2024-02-11 09:10:24.676880     USA                      1\n",
      "3     13F98B34 2024-02-01 12:05:38.203954     PRT                      1\n",
      "4     13F98E1G 2024-02-02 12:15:02.136571     DJI                      8\n",
      "...        ...                        ...     ...                    ...\n",
      "4331  9PFD1E4D 2024-04-05 08:15:03.533333     USA                      2\n",
      "4332  9QFE51C5 2024-05-20 08:15:02.720665     NPL                      1\n",
      "4333  9QG43163 2024-02-05 08:15:02.332097     USA                     12\n",
      "4334  9TG483G9 2024-03-09 19:26:23.757586     JPN                      3\n",
      "4335  9YG30GB1 2024-03-27 08:15:03.101694     USA                      5\n",
      "\n",
      "[4336 rows x 4 columns]\n",
      "\n",
      "Modified Max Streak Data:\n",
      "       user_id  max_streak country_code\n",
      "0     13F987F6         1.0          CHN\n",
      "1     13F98A7A         1.0          CAN\n",
      "2     13F98AHC         1.0          USA\n",
      "3     13F98B34         1.0          PRT\n",
      "4     13F98E1G         1.0          DJI\n",
      "...        ...         ...          ...\n",
      "4489  9PFD1E4D         1.0          USA\n",
      "4490  9QFE51C5         1.0          NPL\n",
      "4491  9QG43163         3.0          USA\n",
      "4492  9TG483G9         1.0          JPN\n",
      "4493  9YG30GB1         2.0          USA\n",
      "\n",
      "[4494 rows x 3 columns]\n",
      "\n",
      "Modified Free Test Usage Data:\n",
      "       user_id  free_test_count country_code\n",
      "0     13F987F6                1          CHN\n",
      "1     13F98E1G                4          DJI\n",
      "2     13F98G1H                1          IND\n",
      "3     13F99B6G                1          IND\n",
      "4     13F99GED                4          DOM\n",
      "...        ...              ...          ...\n",
      "1916  93FBBE62                1          DEU\n",
      "1917  9KDB1803                1          USA\n",
      "1918  9PFD1E4D                1          USA\n",
      "1919  9TG483G9                1          JPN\n",
      "1920  9YG30GB1                3          USA\n",
      "\n",
      "[1921 rows x 3 columns]\n",
      "\n",
      "Aggregated Free Test Section Data:\n",
      "       user_id country_code  free_test_section_count\n",
      "0     13F987F6          CHN                        1\n",
      "1     13F98E1G          DJI                        3\n",
      "2     13F98G1H          IND                        1\n",
      "3     13F99B6G          IND                        4\n",
      "4     13F99GED          DOM                        8\n",
      "...        ...          ...                      ...\n",
      "1797  9KDB1803          USA                        4\n",
      "1798  9PFD1E4D          USA                        2\n",
      "1799  9QG43163          USA                        4\n",
      "1800  9TG483G9          JPN                        5\n",
      "1801  9YG30GB1          USA                        4\n",
      "\n",
      "[1802 rows x 3 columns]\n",
      "\n",
      "Aggregated Page View Data:\n",
      "       user_id country_code  page_view_count\n",
      "0     13F987F6          CHN              133\n",
      "1     13F98A7A          CAN                1\n",
      "2     13F98AHC          USA                1\n",
      "3     13F98B34          PRT               17\n",
      "4     13F98E1G          DJI              280\n",
      "...        ...          ...              ...\n",
      "4948  9PFD1E4D          USA               94\n",
      "4949  9QFE51C5          NPL               32\n",
      "4950  9QG43163          USA              417\n",
      "4951  9TG483G9          JPN              241\n",
      "4952  9YG30GB1          USA              136\n",
      "\n",
      "[4953 rows x 3 columns]\n",
      "\n",
      "Aggregated Paid Prep Data:\n",
      "       user_id country_code  paid_prep_count\n",
      "0     13F98B34          PRT                1\n",
      "1     13F98E1G          DJI                1\n",
      "2     13F9915D          IND                2\n",
      "3     13F995H2          IND                1\n",
      "4     13F998FB          USA                1\n",
      "...        ...          ...              ...\n",
      "1943  93FBBE62          DEU                2\n",
      "1944  93FBD330          SLV                1\n",
      "1945  9KDB1803          USA                2\n",
      "1946  9QFE51C5          NPL                2\n",
      "1947  9QG43163          USA                2\n",
      "\n",
      "[1948 rows x 3 columns]\n",
      "\n",
      "Aggregated View Feedback Data:\n",
      "       user_id country_code  view_feedback_count\n",
      "0     13F987F6          CHN                    3\n",
      "1     13F98B34          PRT                    1\n",
      "2     13F98E1G          DJI                    6\n",
      "3     13F98G1H          IND                    2\n",
      "4     13F995H2          IND                    5\n",
      "...        ...          ...                  ...\n",
      "2056  9KDB1803          USA                    2\n",
      "2057  9NEF23EB          IND                    1\n",
      "2058  9QG43163          USA                   39\n",
      "2059  9TG483G9          JPN                   13\n",
      "2060  9YG30GB1          USA                    2\n",
      "\n",
      "[2061 rows x 3 columns]\n",
      "\n",
      "Aggregated View Plan Data:\n",
      "       user_id country_code  view_plan_count\n",
      "0     13F98AHC          USA                1\n",
      "1     13F98E1G          DJI                3\n",
      "2     13F9915D          IND                1\n",
      "3     13F995H2          IND                2\n",
      "4     13F99DGC          DEU                1\n",
      "...        ...          ...              ...\n",
      "1551  93FBBE62          DEU                4\n",
      "1552  9JG47G9D          IND                1\n",
      "1553  9KDB1803          USA                1\n",
      "1554  9NEF23EB          IND                2\n",
      "1555  9PFD1E4D          USA                3\n",
      "\n",
      "[1556 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "!pip install PyAthena[SQLAlchemy]\n",
    "!aws configure get region\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "!pip install CurrencyConverter\n",
    "import tr_dash_util as util\n",
    "import numpy as np \n",
    "import html \n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from currency_converter import CurrencyConverter, ECB_URL\n",
    "from tr_dash_util import upload_dataframe_to_s3, extract_eReg_orders, clean_TR_dataframe, fetch_data_from_s3\n",
    "from functools import reduce\n",
    "import pytz\n",
    "\n",
    "import boto3\n",
    "# Set time and environment\n",
    "today = datetime.now() - timedelta(days=1)\n",
    "prod = True\n",
    "\n",
    "# S3 path for Athena\n",
    "s3_staging_dir = \"s3://ets-aws-plalab-dii-prod-analyticsbucket-1ktrlhzbrcbkb/athena_query_results/\"\n",
    "\n",
    "# Athena connection string\n",
    "connection_string = f\"awsathena+rest://:@athena.us-east-1.amazonaws.com:443/labsprodeventsdatabase-x806vjuzpbrd?s3_staging_dir={s3_staging_dir}\"\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "def set_date_range(start_date='2024-02-01', end_date='2024-05-27'):\n",
    "    \"\"\"\n",
    "    Returns the start and end dates as a tuple.\n",
    "    \"\"\"\n",
    "    return start_date, end_date\n",
    "\n",
    "def pull_parquet_files_pandas(file_name, bucket_name='ets-aws-plalab-dii-prod-analyticsbucket-1ktrlhzbrcbkb'):\n",
    "    \"\"\"\n",
    "    Pulls Parquet files from a specified S3 bucket and concatenates them into a single DataFrame.\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    prefix = f'sagemaker/test_ready/intermediate_files/file_name={file_name}/'\n",
    "\n",
    "    def list_parquet_files(bucket, prefix):\n",
    "        \"\"\"List all Parquet files within the structured partitioning scheme in S3.\"\"\"\n",
    "        file_paths = []\n",
    "        paginator = s3_client.get_paginator('list_objects_v2')\n",
    "        for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "            for content in page.get('Contents', []):\n",
    "                key = content.get('Key')\n",
    "                if key.endswith('.parquet'):  # Ensure we're only capturing Parquet files\n",
    "                    file_paths.append(f\"s3://{bucket}/{key}\")\n",
    "        return file_paths\n",
    "\n",
    "    file_paths = list_parquet_files(bucket_name, prefix)\n",
    "    df = pd.DataFrame()\n",
    "    for file_path in file_paths:\n",
    "        temp_df = pd.read_parquet(file_path, engine='pyarrow')\n",
    "        df = pd.concat([df, temp_df], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def pull_parquet_files_pandas_Target(file_name, bucket_name='ets-dii-testready-ds-analytics'):\n",
    "    \"\"\"\n",
    "    Pulls Parquet files from a specified S3 bucket and concatenates them into a single DataFrame.\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    prefix = f'Eben/Features/file_name={file_name}/'\n",
    "\n",
    "    def list_parquet_files(bucket, prefix):\n",
    "        \"\"\"List all Parquet files within the structured partitioning scheme in S3.\"\"\"\n",
    "        file_paths = []\n",
    "        paginator = s3_client.get_paginator('list_objects_v2')\n",
    "        for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "            for content in page.get('Contents', []):\n",
    "                key = content.get('Key')\n",
    "                if key.endswith('.parquet'):  # Ensure we're only capturing Parquet files\n",
    "                    file_paths.append(f\"s3://{bucket}/{key}\")\n",
    "        return file_paths\n",
    "\n",
    "    file_paths = list_parquet_files(bucket_name, prefix)\n",
    "    df = pd.DataFrame()\n",
    "    for file_path in file_paths:\n",
    "        temp_df = pd.read_parquet(file_path, engine='pyarrow')\n",
    "        df = pd.concat([df, temp_df], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def pull_parquet_files_pandas_features(file_name, bucket_name='ets-dii-testready-ds-analytics'):\n",
    "    \"\"\"\n",
    "    Pulls Parquet files from a specified S3 bucket and concatenates them into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The name of the file (or file identifier) to search for in the S3 bucket.\n",
    "        bucket_name (str, optional): The name of the S3 bucket to search in. Defaults to 'ets-dii-testready-ds-analytics'.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the concatenated data from all found Parquet files.\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    prefix = f'features/file_name={file_name}/'\n",
    "\n",
    "    def list_parquet_files(bucket, prefix):\n",
    "        \"\"\"List all Parquet files within the structured partitioning scheme in S3.\"\"\"\n",
    "        file_paths = []\n",
    "        paginator = s3_client.get_paginator('list_objects_v2')\n",
    "        for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "            for content in page.get('Contents', []):\n",
    "                key = content.get('Key')\n",
    "                if key.endswith('.parquet'):  # Ensure we're only capturing Parquet files\n",
    "                    file_paths.append(f\"s3://{bucket}/{key}\")\n",
    "        return file_paths\n",
    "\n",
    "    file_paths = list_parquet_files(bucket_name, prefix)\n",
    "\n",
    "    # Initialize an empty DataFrame\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # Read each Parquet file into a DataFrame\n",
    "        temp_df = pd.read_parquet(file_path, engine='pyarrow')\n",
    "        # Concatenate to the main DataFrame\n",
    "        df = pd.concat([df, temp_df], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def get_first_purchase_dates(orders_df):\n",
    "    \"\"\"\n",
    "    Extracts the first purchase date for each user.\n",
    "    Args:\n",
    "        orders_df (pd.DataFrame): DataFrame containing order data with 'user_id' and 'order_sumitted'.\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with 'user_id' and 'first_purchase_date'.\n",
    "    \"\"\"\n",
    "    # Convert order_sumitted to datetime\n",
    "    orders_df['order_sumitted'] = pd.to_datetime(orders_df['order_sumitted'], unit='s', errors='coerce')\n",
    "    \n",
    "    # Filter out invalid dates (NaT values)\n",
    "    #orders_df = orders_df.dropna(subset=['order_sumitted'])\n",
    "    \n",
    "    # Get the first purchase date for each user\n",
    "    first_purchase_dates = orders_df.groupby('user_id')['order_sumitted'].min().reset_index()\n",
    "    first_purchase_dates.rename(columns={'order_sumitted': 'first_purchase_date'}, inplace=True)\n",
    "    \n",
    "    return first_purchase_dates\n",
    "\n",
    "def filter_events_before_first_purchase(events_df, first_purchase_dates):\n",
    "    \"\"\"\n",
    "    Filters events to include only those that occurred before the user's first purchase.\n",
    "    Args:\n",
    "        events_df (pd.DataFrame): DataFrame containing event data with 'user_id' and 'date'.\n",
    "        first_purchase_dates (pd.DataFrame): DataFrame with 'user_id' and 'first_purchase_date'.\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame.\n",
    "    \"\"\"\n",
    "    events_df['date'] = pd.to_datetime(events_df['date'])\n",
    "    merged_df = events_df.merge(first_purchase_dates, on='user_id', how='left')\n",
    "    filtered_df = merged_df[merged_df['date'] < merged_df['first_purchase_date']].copy()\n",
    "    return filtered_df\n",
    "\n",
    "def aggregate_study_plans(data, first_purchase_dates, date_range):\n",
    "    \"\"\"\n",
    "    Aggregates study plan data at the user level within a specified date range, excluding events after first purchase.\n",
    "    Args:\n",
    "        data (pd.DataFrame): The original DataFrame containing study plan data with columns 'date', 'user_id', 'country', 'plan_num'.\n",
    "        first_purchase_dates (pd.DataFrame): DataFrame with 'user_id' and 'first_purchase_date'.\n",
    "        date_range (tuple): Tuple containing the start and end dates.\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame aggregated at the user level with columns 'user_id', 'first_date', 'country', and 'number_of_study_plans'.\n",
    "    \"\"\"\n",
    "    start_date, end_date = date_range\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    filtered_data = data[(data['date'] >= start_date) & (data['date'] <= end_date)]\n",
    "    filtered_data = filter_events_before_first_purchase(filtered_data, first_purchase_dates)\n",
    "    user_level_data = filtered_data.groupby('user_id').agg({\n",
    "        'date': 'min',\n",
    "        'country': 'first',\n",
    "        'plan_num': 'count'\n",
    "    }).reset_index()\n",
    "    user_level_data.rename(columns={'date': 'first_date', 'plan_num': 'number_of_study_plans'}, inplace=True)\n",
    "    return user_level_data\n",
    "\n",
    "def max_streak_within_date_range(data, first_purchase_dates, date_range):\n",
    "    \"\"\"\n",
    "    Calculates the maximum streak for each user within a specified date range, excluding events after first purchase.\n",
    "    Args:\n",
    "        data (pd.DataFrame): The original DataFrame containing streak data with columns 'date', 'user_id', 'max_days_in_a_row', 'country_code'.\n",
    "        first_purchase_dates (pd.DataFrame): DataFrame with 'user_id' and 'first_purchase_date'.\n",
    "        date_range (tuple): Tuple containing the start and end dates.\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame aggregated at the user level with columns 'user_id', 'country_code', and 'max_streak'.\n",
    "    \"\"\"\n",
    "    start_date, end_date = date_range\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    filtered_data = data[(data['date'] >= start_date) & (data['date'] <= end_date)]\n",
    "    filtered_data = filter_events_before_first_purchase(filtered_data, first_purchase_dates)\n",
    "    filtered_data.loc[:, 'country_code'] = filtered_data['country_code'].fillna('Unknown')\n",
    "    user_level_data = filtered_data.groupby('user_id').agg({\n",
    "        'max_days_in_a_row': 'max',\n",
    "        'country_code': 'first'\n",
    "    }).reset_index()\n",
    "    user_level_data.rename(columns={'max_days_in_a_row': 'max_streak'}, inplace=True)\n",
    "    return user_level_data\n",
    "\n",
    "def count_free_test_usage(data, first_purchase_dates, date_range):\n",
    "    \"\"\"\n",
    "    Counts the number of times each user has used the 'Free' test type within a specified date range, excluding events after first purchase.\n",
    "    Args:\n",
    "        data (pd.DataFrame): The original DataFrame containing prep data with columns 'date', 'user_id', 'country_code', 'prep_type', 'prep_time'.\n",
    "        first_purchase_dates (pd.DataFrame): DataFrame with 'user_id' and 'first_purchase_date'.\n",
    "        date_range (tuple): Tuple containing the start and end dates.\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame aggregated at the user level with columns 'user_id', 'country_code', and 'free_test_count'.\n",
    "    \"\"\"\n",
    "    start_date, end_date = date_range\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    filtered_data = data[(data['date'] >= start_date) & (data['date'] <= end_date)]\n",
    "    filtered_data = filter_events_before_first_purchase(filtered_data, first_purchase_dates)\n",
    "    free_test_data = filtered_data[filtered_data['prep_type'] == 'Free']\n",
    "    free_test_count = free_test_data.groupby('user_id').agg({\n",
    "        'prep_type': 'count',\n",
    "        'country_code': 'first'\n",
    "    }).reset_index()\n",
    "    free_test_count.rename(columns={'prep_type': 'free_test_count'}, inplace=True)\n",
    "    return free_test_count\n",
    "\n",
    "def aggregate_free_test_section(data, first_purchase_dates, date_range):\n",
    "    \"\"\"\n",
    "    Aggregates free test section data at the user level within a specified date range, excluding events after first purchase.\n",
    "    Args:\n",
    "        data (pd.DataFrame): The original DataFrame containing free test section data with columns 'date', 'user_id', 'event_name', 'skill', 'country_code', 'backend_timestamp', 'session'.\n",
    "        first_purchase_dates (pd.DataFrame): DataFrame with 'user_id' and 'first_purchase_date'.\n",
    "        date_range (tuple): Tuple containing the start and end dates.\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame aggregated at the user level with columns 'user_id', 'country_code', and 'free_test_section_count'.\n",
    "    \"\"\"\n",
    "    start_date, end_date = date_range\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    filtered_data = data[(data['date'] >= start_date) & (data['date'] <= end_date)]\n",
    "    filtered_data = filter_events_before_first_purchase(filtered_data, first_purchase_dates)\n",
    "    user_level_data = filtered_data.groupby('user_id').agg({\n",
    "        'country_code': 'first',\n",
    "        'event_name': 'count'\n",
    "    }).reset_index()\n",
    "    user_level_data.rename(columns={'event_name': 'free_test_section_count'}, inplace=True)\n",
    "    return user_level_data\n",
    "\n",
    "def aggregate_page_view(data, first_purchase_dates, date_range):\n",
    "    \"\"\"\n",
    "    Aggregates page view data at the user level within a specified date range, excluding events after first purchase.\n",
    "    Args:\n",
    "        data (pd.DataFrame): The original DataFrame containing page view data with columns 'date', 'user_id', 'event_name', 'screen', 'country_code', 'backend_timestamp', 'session'.\n",
    "        first_purchase_dates (pd.DataFrame): DataFrame with 'user_id' and 'first_purchase_date'.\n",
    "        date_range (tuple): Tuple containing the start and end dates.\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame aggregated at the user level with columns 'user_id', 'country_code', and 'page_view_count'.\n",
    "    \"\"\"\n",
    "    start_date, end_date = date_range\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    filtered_data = data[(data['date'] >= start_date) & (data['date'] <= end_date)]\n",
    "    filtered_data = filter_events_before_first_purchase(filtered_data, first_purchase_dates)\n",
    "    user_level_data = filtered_data.groupby('user_id').agg({\n",
    "        'country_code': 'first',\n",
    "        'event_name': 'count'\n",
    "    }).reset_index()\n",
    "    user_level_data.rename(columns={'event_name': 'page_view_count'}, inplace=True)\n",
    "    return user_level_data\n",
    "\n",
    "def aggregate_paid_prep(data, first_purchase_dates, date_range):\n",
    "    \"\"\"\n",
    "    Aggregates paid prep data at the user level within a specified date range, excluding events after first purchase.\n",
    "    Args:\n",
    "        data (pd.DataFrame): The original DataFrame containing paid prep data with columns 'date', 'user_id', 'event_name', 'prep_type', 'au_num', 'country_code', 'backend_timestamp', 'session'.\n",
    "        first_purchase_dates (pd.DataFrame): DataFrame with 'user_id' and 'first_purchase_date'.\n",
    "        date_range (tuple): Tuple containing the start and end dates.\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame aggregated at the user level with columns 'user_id', 'country_code', and 'paid_prep_count'.\n",
    "    \"\"\"\n",
    "    start_date, end_date = date_range\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    filtered_data = data[(data['date'] >= start_date) & (data['date'] <= end_date)]\n",
    "    filtered_data = filter_events_before_first_purchase(filtered_data, first_purchase_dates)\n",
    "    user_level_data = filtered_data.groupby('user_id').agg({\n",
    "        'country_code': 'first',\n",
    "        'event_name': 'count'\n",
    "    }).reset_index()\n",
    "    user_level_data.rename(columns={'event_name': 'paid_prep_count'}, inplace=True)\n",
    "    return user_level_data\n",
    "\n",
    "def aggregate_view_feedback(data, first_purchase_dates, date_range):\n",
    "    \"\"\"\n",
    "    Aggregates view feedback data at the user level within a specified date range, excluding events after first purchase.\n",
    "    Args:\n",
    "        data (pd.DataFrame): The original DataFrame containing view feedback data with columns 'date', 'user_id', 'event_name', 'prep_type', 'skill', 'country_code', 'backend_timestamp', 'session'.\n",
    "        first_purchase_dates (pd.DataFrame): DataFrame with 'user_id' and 'first_purchase_date'.\n",
    "        date_range (tuple): Tuple containing the start and end dates.\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame aggregated at the user level with columns 'user_id', 'country_code', and 'view_feedback_count'.\n",
    "    \"\"\"\n",
    "    start_date, end_date = date_range\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    filtered_data = data[(data['date'] >= start_date) & (data['date'] <= end_date)]\n",
    "    filtered_data = filter_events_before_first_purchase(filtered_data, first_purchase_dates)\n",
    "    user_level_data = filtered_data.groupby('user_id').agg({\n",
    "        'country_code': 'first',\n",
    "        'event_name': 'count'\n",
    "    }).reset_index()\n",
    "    user_level_data.rename(columns={'event_name': 'view_feedback_count'}, inplace=True)\n",
    "    return user_level_data\n",
    "\n",
    "def aggregate_view_plan(data, first_purchase_dates, date_range):\n",
    "    \"\"\"\n",
    "    Aggregates view plan data at the user level within a specified date range, excluding events after first purchase.\n",
    "    Args:\n",
    "        data (pd.DataFrame): The original DataFrame containing view plan data with columns 'date', 'user_id', 'event_name', 'country_code', 'backend_timestamp', 'session'.\n",
    "        first_purchase_dates (pd.DataFrame): DataFrame with 'user_id' and 'first_purchase_date'.\n",
    "        date_range (tuple): Tuple containing the start and end dates.\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame aggregated at the user level with columns 'user_id', 'country_code', and 'view_plan_count'.\n",
    "    \"\"\"\n",
    "    start_date, end_date = date_range\n",
    "    data['date'] = pd.to_datetime(data['date'])\n",
    "    filtered_data = data[(data['date'] >= start_date) & (data['date'] <= end_date)]\n",
    "    filtered_data = filter_events_before_first_purchase(filtered_data, first_purchase_dates)\n",
    "    user_level_data = filtered_data.groupby('user_id').agg({\n",
    "        'country_code': 'first',\n",
    "        'event_name': 'count'\n",
    "    }).reset_index()\n",
    "    user_level_data.rename(columns={'event_name': 'view_plan_count'}, inplace=True)\n",
    "    return user_level_data\n",
    "\n",
    "def process_data():\n",
    "    \"\"\"\n",
    "    Processes data by pulling Parquet files from S3, aggregating study plan data, calculating maximum streaks,\n",
    "    counting free test usage, and aggregating free test section, page view, paid prep, view feedback, and view plan data for users within a specified date range.\n",
    "    Saves the results as Parquet files.\n",
    "    \"\"\"\n",
    "    date_range = set_date_range()\n",
    "    \n",
    "    # Pull data from S3\n",
    "    study_plan_data = pull_parquet_files_pandas(file_name='personalized_plan')\n",
    "    max_streak_data = pull_parquet_files_pandas(file_name='user_max_streak')\n",
    "    free_test_data = pull_parquet_files_pandas(file_name='prep_time')\n",
    "    free_test_section_data = pull_parquet_files_pandas_features(file_name='free_test_section')\n",
    "    page_view_data = pull_parquet_files_pandas_features(file_name='page_view')\n",
    "    paid_prep_data = pull_parquet_files_pandas_features(file_name='paid_prep')\n",
    "    view_feedback_data = pull_parquet_files_pandas_features(file_name='view_feedback')\n",
    "    view_plan_data = pull_parquet_files_pandas_features(file_name='view_plan')\n",
    "    target_data = pull_parquet_files_pandas_Target(file_name='Target')\n",
    "\n",
    "    # Get first purchase dates\n",
    "    first_purchase_dates = get_first_purchase_dates(target_data)\n",
    "\n",
    "    # Process data\n",
    "    modified_aggregated_study_plan_data = aggregate_study_plans(study_plan_data, first_purchase_dates, date_range)\n",
    "    modified_max_streak_user_data = max_streak_within_date_range(max_streak_data, first_purchase_dates, date_range)\n",
    "    modified_free_test_usage_data = count_free_test_usage(free_test_data, first_purchase_dates, date_range)\n",
    "    aggregated_free_test_section_data = aggregate_free_test_section(free_test_section_data, first_purchase_dates, date_range)\n",
    "    aggregated_page_view_data = aggregate_page_view(page_view_data, first_purchase_dates, date_range)\n",
    "    aggregated_paid_prep_data = aggregate_paid_prep(paid_prep_data, first_purchase_dates, date_range)\n",
    "    aggregated_view_feedback_data = aggregate_view_feedback(view_feedback_data, first_purchase_dates, date_range)\n",
    "    aggregated_view_plan_data = aggregate_view_plan(view_plan_data, first_purchase_dates, date_range)\n",
    "\n",
    "    # Save results to Parquet files\n",
    "    modified_aggregated_study_plan_data.to_parquet('modified_aggregated_study_plan_data.parquet', index=False)\n",
    "    modified_max_streak_user_data.to_parquet('modified_max_streak_user_data.parquet', index=False)\n",
    "    modified_free_test_usage_data.to_parquet('modified_free_test_usage_data.parquet', index=False)\n",
    "    aggregated_free_test_section_data.to_parquet('aggregated_free_test_section_data.parquet', index=False)\n",
    "    aggregated_page_view_data.to_parquet('aggregated_page_view_data.parquet', index=False)\n",
    "    aggregated_paid_prep_data.to_parquet('aggregated_paid_prep_data.parquet', index=False)\n",
    "    aggregated_view_feedback_data.to_parquet('aggregated_view_feedback_data.parquet', index=False)\n",
    "    aggregated_view_plan_data.to_parquet('aggregated_view_plan_data.parquet', index=False)\n",
    "\n",
    "    # Output results\n",
    "    print(\"Modified Aggregated Study Plan Data:\")\n",
    "    print(modified_aggregated_study_plan_data)\n",
    "    print(\"\\nModified Max Streak Data:\")\n",
    "    print(modified_max_streak_user_data)\n",
    "    print(\"\\nModified Free Test Usage Data:\")\n",
    "    print(modified_free_test_usage_data)\n",
    "    print(\"\\nAggregated Free Test Section Data:\")\n",
    "    print(aggregated_free_test_section_data)\n",
    "    print(\"\\nAggregated Page View Data:\")\n",
    "    print(aggregated_page_view_data)\n",
    "    print(\"\\nAggregated Paid Prep Data:\")\n",
    "    print(aggregated_paid_prep_data)\n",
    "    print(\"\\nAggregated View Feedback Data:\")\n",
    "    print(aggregated_view_feedback_data)\n",
    "    print(\"\\nAggregated View Plan Data:\")\n",
    "    print(aggregated_view_plan_data)\n",
    "\n",
    "# Call the process_data function to execute the workflow\n",
    "process_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
