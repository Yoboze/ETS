{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a1001ac-44f0-440a-8875-903877b5dc78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching order data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4170/1746796442.py:102: FutureWarning: In a future version of pandas, parsing datetimes with mixed time zones will raise an error unless `utc=True`. Please specify `utc=True` to opt in to the new behaviour and silence this warning. To create a `Series` with mixed offsets and `object` dtype, please use `apply` and `datetime.datetime.strptime`\n",
      "  df[date_column] = pd.to_datetime(df[date_column], errors='coerce')\n",
      "/tmp/ipykernel_4170/1746796442.py:126: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['skuid'] = pd.to_numeric(df['skuid'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order data fetched and saved to CSV.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine\n",
    "import boto3\n",
    "import pytz\n",
    "\n",
    "# Define constants and configurations\n",
    "S3_STAGING_DIR = \"s3://ets-aws-plalab-dii-prod-analyticsbucket-1ktrlhzbrcbkb/athena_query_results/\"\n",
    "ATHENA_REGION = \"us-east-1\"\n",
    "START_DATE = pytz.utc.localize(datetime(2024, 2, 1))\n",
    "END_DATE = pytz.utc.localize(datetime(2024, 5, 27))\n",
    "\n",
    "# SKU lists\n",
    "NON_PREP_SKUS = [\n",
    "    '-2002', '-2001', '-1111', '4001', '4002', '4003', '4004', '4005', '4006', '4007', '4008', '4009', '4010', '4011',\n",
    "    '4012', '4013', '4014', '4015', '4016', '4017', '4018', '4019', '4020', '4028', '4034', '4035', '4091', '4092', '4093',\n",
    "    '4321', '5000', '5001', '5002', '5003', '5004', '5005', '5006', '5007', '5009', '5010', '5020', '5021', '5022', '5023',\n",
    "    '5024', '5025', '5026', '5027', '5028', '5029', '5030', '5031', '5032', '5033', '5035', '5037', '5040', '5041', '5042',\n",
    "    '5043', '5044', '5045', '5046', '5047', '5048', '5049', '5050', '5051', '5052', '5053', '5054', '5055', '5056', '5057',\n",
    "    '5058', '5059', '5060', '5061', '5062', '5063', '5064', '5065', '5066', '5067', '5068', '5069', '5070', '5071', '5072',\n",
    "    '5073', '5074', '5075', '5076', '5077', '5078', '5079', '5080', '5081', '5082', '5083', '5084', '5085', '5086', '5087',\n",
    "    '5088', '5089', '5091', '5093', '5094', '5095', '5096', '5097', '5098', '5099', '5100', '5101', '5102', '5103', '5104',\n",
    "    '5105', '5106', '5107', '5108', '5109', '5110', '5111', '5112', '5113', '5114', '5115', '5116', '5117', '5118', '5119',\n",
    "    '5120', '5121', '5122', '5123', '5124', '5125', '5126', '5127', '5128', '5129', '5130', '5131', '5132', '5133', '5134',\n",
    "    '5135', '5136', '5137', '5138', '5139', '5140', '5142', '5143', '5144', '5145', '5146', '5147', '5148', '5149', '5150',\n",
    "    '5151', '5152', '5154', '5155', '5156', '5157', '5159', '5160', '5161', '5162', '5163', '5164', '5165', '5166', '5167',\n",
    "    '5168', '5169', '5170', '5171', '5172', '5173', '5174', '5175', '5176', '5177', '5178', '5179', '5180', '5181', '5182',\n",
    "    '5183', '5184', '5185', '5186', '5187', '5188', '5189', '5190', '5191', '5192', '5193', '5194', '5195', '5196', '5197',\n",
    "    '5198', '5199', '5200', '5600', '5601', '5602', '5603', '5604', '5605', '5606', '5607', '5608', '5609', '5610', '5611',\n",
    "    '5612', '5613', '5614', '5615', '5616', '5617', '5618', '5619', '5620', '5621', '5622', '5624', '5625', '5626', '5627',\n",
    "    '5628', '5629', '5630', '5631', '5632', '5633', '5634', '5635', '5636', '5637', '5638', '5639', '5640', '5641', '5642',\n",
    "    '5643', '5644', '5645', '5646', '5647', '5648', '5649', '5650', '5651', '5652', '5653', '5654', '5655', '5656', '5657',\n",
    "    '5658', '5659', '5660', '5661', '5662', '5663', '5664', '5665', '5666', '5667', '5668', '5669', '5670', '5671', '5672',\n",
    "    '5673', '5674', '5675', '5676', '5677', '5678', '5679', '5680', '5681', '5682', '5683', '5684', '5685', '5686', '5687',\n",
    "    '5688', '5689', '5690', '5691', '5692', '5693', '5694', '5695', '5696', '6001', '6002', '6003', '6004', '6005', '6009',\n",
    "    '6010', '6011', '6012', '6013', '6014', '6015', '6016', '6017', '6018', '100155', '100156', '100206', '100207'\n",
    "]\n",
    "SKU_ID_REGISTRATION = ['4001', '4017', '6010', '6017']\n",
    "\n",
    "def create_engine_connection():\n",
    "    \"\"\"\n",
    "    Create a connection to the Amazon Athena database using SQLAlchemy.\n",
    "\n",
    "    Returns:\n",
    "        engine (SQLAlchemy engine): The engine object to interact with the Athena database.\n",
    "    \"\"\"\n",
    "    connection_string = f\"awsathena+rest://:@athena.{ATHENA_REGION}.amazonaws.com:443/labsprodeventsdatabase-x806vjuzpbrd?s3_staging_dir={S3_STAGING_DIR}\"\n",
    "    engine = create_engine(connection_string)\n",
    "    return engine\n",
    "\n",
    "def pull_parquet_files_pandas_features(file_name, bucket_name='ets-dii-testready-ds-analytics'):\n",
    "    \"\"\"\n",
    "    Pulls Parquet files from a specified S3 bucket and concatenates them into a single DataFrame.\n",
    "\n",
    "    Args:\n",
    "        file_name (str): The name of the file (or file identifier) to search for in the S3 bucket.\n",
    "        bucket_name (str, optional): The name of the S3 bucket to search in. Defaults to 'ets-dii-testready-ds-analytics'.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame containing the concatenated data from all found Parquet files.\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    prefix = f'features/file_name={file_name}/'\n",
    "\n",
    "    def list_parquet_files(bucket, prefix):\n",
    "        \"\"\"List all Parquet files within the structured partitioning scheme in S3.\"\"\"\n",
    "        file_paths = []\n",
    "        paginator = s3_client.get_paginator('list_objects_v2')\n",
    "        for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "            for content in page.get('Contents', []):\n",
    "                key = content.get('Key')\n",
    "                if key.endswith('.parquet'):  # Ensure we're only capturing Parquet files\n",
    "                    file_paths.append(f\"s3://{bucket}/{key}\")\n",
    "        return file_paths\n",
    "\n",
    "    file_paths = list_parquet_files(bucket_name, prefix)\n",
    "\n",
    "    # Initialize an empty DataFrame\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # Read each Parquet file into a DataFrame\n",
    "        temp_df = pd.read_parquet(file_path, engine='pyarrow')\n",
    "        # Concatenate to the main DataFrame\n",
    "        df = pd.concat([df, temp_df], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def filter_dataframe_by_date(df, start_date, end_date, date_column='submit_datetime'):\n",
    "    \"\"\"\n",
    "    Filters a DataFrame to include only rows where the date in date_column is between start_date and end_date.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame to filter.\n",
    "        start_date (datetime): The start date for the filter.\n",
    "        end_date (datetime): The end date for the filter.\n",
    "        date_column (str, optional): The name of the column containing datetime values. Defaults to 'submit_datetime'.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The filtered DataFrame.\n",
    "    \"\"\"\n",
    "    df[date_column] = pd.to_datetime(df[date_column], errors='coerce')\n",
    "    # Ensure all datetimes are timezone-aware and converted to UTC\n",
    "    df[date_column] = df[date_column].apply(lambda x: x.tz_convert('UTC') if x.tzinfo is not None else x.tz_localize('UTC'))\n",
    "    filtered_df = df[(df[date_column] >= start_date) & (df[date_column] <= end_date)]\n",
    "    return filtered_df\n",
    "\n",
    "def process_order_data(df, non_prep_skus, sku_id_registration):\n",
    "    \"\"\"\n",
    "    Processes the order data to filter out non-prep SKUs and identify registration markers using vectorized methods.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): The DataFrame containing the raw order data.\n",
    "        non_prep_skus (list): List of SKUs to be considered as non-prep.\n",
    "        sku_id_registration (list): List of SKU IDs used for registration.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The processed DataFrame with filtered order data.\n",
    "    \"\"\"\n",
    "    # Convert SKU lists to sets for faster membership checking\n",
    "    non_prep_skus_set = set(map(int, [sku for sku in non_prep_skus if sku.isdigit()]))\n",
    "    sku_id_registration_set = set(map(int, [sku for sku in sku_id_registration if sku.isdigit()]))\n",
    "    sku_id_fees = non_prep_skus_set - sku_id_registration_set\n",
    "\n",
    "    # Ensure 'skuid' is numeric and filter out invalid skus\n",
    "    df['skuid'] = pd.to_numeric(df['skuid'], errors='coerce')\n",
    "    df = df.dropna(subset=['skuid'])\n",
    "    df['skuid'] = df['skuid'].astype(int)\n",
    "\n",
    "    # Filter out sku_id_fees and keep valid SKUs\n",
    "    df = df[~df['skuid'].isin(sku_id_fees)]\n",
    "\n",
    "    # Add registration_marker column\n",
    "    df['registration_marker'] = df['skuid'].isin(sku_id_registration_set).astype(int)\n",
    "\n",
    "    # Convert 'submit_datetime' to datetime and ensure timezone-aware\n",
    "    df['submit_datetime'] = pd.to_datetime(df['submit_datetime'], errors='coerce')\n",
    "    df['submit_datetime'] = df['submit_datetime'].apply(lambda x: x.tz_convert('UTC') if x.tzinfo is not None else x.tz_localize('UTC'))\n",
    "\n",
    "    # Filter out orders with 'fulfillmenttype' containing 'RESCHEDULE'\n",
    "    if 'fulfillmenttype' in df.columns:\n",
    "        df['fulfillmenttype'] = df['fulfillmenttype'].astype(str)\n",
    "        df = df[~df['fulfillmenttype'].str.contains('RESCHEDULE', case=False, na=False)]\n",
    "\n",
    "    # Filter to keep only rows where registration_marker is 0\n",
    "    df = df[df['registration_marker'] == 0]\n",
    "\n",
    "    # Select only the required columns\n",
    "    df = df[['user_id', 'order_id', 'submit_datetime', 'skuid', 'registration_marker']]\n",
    "\n",
    "    # Sort and drop duplicates to keep only the first order for each user\n",
    "    df.sort_values(by=['user_id', 'submit_datetime'], ascending=[True, True], inplace=True)\n",
    "    df = df.drop_duplicates(subset='user_id', keep='first')\n",
    "\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the data fetching, processing, and saving operations.\n",
    "    \"\"\"\n",
    "    print(\"Fetching order data...\")\n",
    "    parquet_data = pull_parquet_files_pandas_features(file_name='eReg_order_items')\n",
    "    \n",
    "    # Filter the data by the specified date range\n",
    "    filtered_data = filter_dataframe_by_date(parquet_data, START_DATE, END_DATE)\n",
    "    \n",
    "    # Process the filtered order data\n",
    "    processed_order_data = process_order_data(filtered_data, NON_PREP_SKUS, SKU_ID_REGISTRATION)\n",
    "    \n",
    "    # Save the processed order data DataFrame to a CSV file for verification\n",
    "    processed_order_data.to_csv(\"order_data.csv\", index=False)\n",
    "    print(\"Order data fetched and saved to CSV.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
